{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n",
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 30),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor([[-1.2,3.1], [-0.5, 2.3], [-0.2, 1.9], [1.3, -3.2], [1.5, -3.5]])\n",
    "train_y = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "test_X = torch.tensor([[-0.5, 2.0], [0.3, -1.5]])\n",
    "test_y = torch.tensor([0, 1])\n",
    "\n",
    "train_dataset = ToyDataset(train_X, train_y)\n",
    "test_dataset = ToyDataset(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 1.3000, -3.2000],\n",
      "        [-0.5000,  2.3000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.2000,  1.9000]]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx+1}: {batch_X} {batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/005 | Batch 001/002 | Train Loss: 0.77\n",
      "Epoch: 001/005 | Batch 002/002 | Train Loss: 0.53\n",
      "Epoch: 002/005 | Batch 001/002 | Train Loss: 0.24\n",
      "Epoch: 002/005 | Batch 002/002 | Train Loss: 0.15\n",
      "Epoch: 003/005 | Batch 001/002 | Train Loss: 0.00\n",
      "Epoch: 003/005 | Batch 002/002 | Train Loss: 0.02\n",
      "Epoch: 004/005 | Batch 001/002 | Train Loss: 0.01\n",
      "Epoch: 004/005 | Batch 002/002 | Train Loss: 0.00\n",
      "Epoch: 005/005 | Batch 001/002 | Train Loss: 0.00\n",
      "Epoch: 005/005 | Batch 002/002 | Train Loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(input_size=2, output_size=2)\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx+1:03d}/{len(train_loader):03d} | Train Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1143, -4.4012],\n",
      "        [ 2.2527, -3.2476],\n",
      "        [ 1.8369, -2.6932],\n",
      "        [-2.6072,  2.6559],\n",
      "        [-2.8267,  2.8766]], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(train_X.to(\"cuda\"))\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model: NeuralNetwork, data_loader: DataLoader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0.0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(data_loader):\n",
    "        features = features.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = predictions == labels\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()\n",
    "\n",
    "print(compute_accuracy(model, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('layers.0.weight', tensor([[-0.3019,  0.0973],\n",
      "        [-0.3558,  0.2890],\n",
      "        [-0.6030,  0.5213],\n",
      "        [-0.5200, -0.5465],\n",
      "        [-0.4574,  0.3799],\n",
      "        [-0.2740,  0.3343],\n",
      "        [-0.5567, -0.5347],\n",
      "        [-0.2396, -0.1879],\n",
      "        [-0.5362,  0.4429],\n",
      "        [-0.1750,  0.2742],\n",
      "        [ 0.4953,  0.1550],\n",
      "        [ 0.1864,  0.7209],\n",
      "        [-0.3247,  0.2536],\n",
      "        [-0.3240,  0.5583],\n",
      "        [ 0.5893, -0.7109],\n",
      "        [ 0.5970,  0.3706],\n",
      "        [ 0.3037,  0.3125],\n",
      "        [ 0.5897, -0.0972],\n",
      "        [-0.5998, -0.1983],\n",
      "        [-0.5086,  0.1027],\n",
      "        [-0.0954, -0.4681],\n",
      "        [-0.0715,  0.7096],\n",
      "        [-0.0329, -0.0578],\n",
      "        [-0.1236,  0.1872],\n",
      "        [ 0.6216,  0.4837],\n",
      "        [ 0.2510,  0.1537],\n",
      "        [ 0.1525,  0.3560],\n",
      "        [-0.0808, -0.6816],\n",
      "        [-0.4417,  0.6238],\n",
      "        [ 0.0595, -0.6132]])), ('layers.0.bias', tensor([ 0.1459,  0.5943, -0.6668, -0.4813, -0.2536,  0.0609, -0.1324, -0.0626,\n",
      "        -0.6990,  0.3534,  0.6318,  0.4501, -0.4872, -0.5452,  0.3113,  0.0308,\n",
      "         0.2995, -0.6901, -0.0407,  0.5245,  0.3552,  0.0443,  0.1311, -0.0312,\n",
      "        -0.3643, -0.2672, -0.3915,  0.5942,  0.3981,  0.2589])), ('layers.2.weight', tensor([[    -0.1659,      0.0156,     -0.0051,      0.1984,     -0.1259,\n",
      "             -0.0280,      0.2525,      0.1380,      0.0999,     -0.1240,\n",
      "              0.1748,     -0.3207,      0.0072,     -0.1622,      0.3611,\n",
      "             -0.0976,     -0.0648,      0.2113,      0.0904,     -0.0770,\n",
      "              0.3187,     -0.2374,      0.0976,     -0.0372,     -0.0613,\n",
      "              0.0214,     -0.1482,      0.3410,     -0.1175,      0.4219],\n",
      "        [    -0.0570,      0.0654,      0.1822,     -0.0783,      0.1733,\n",
      "             -0.0909,      0.0805,      0.0715,      0.0508,      0.1441,\n",
      "             -0.0739,      0.0475,      0.0009,     -0.1376,     -0.0443,\n",
      "             -0.1220,      0.0806,      0.0164,      0.0179,     -0.0556,\n",
      "              0.0009,     -0.0572,      0.0524,      0.1772,      0.0276,\n",
      "             -0.0809,     -0.1116,      0.0140,     -0.1359,     -0.1372],\n",
      "        [    -0.1149,     -0.0553,      0.0188,      0.0990,      0.0766,\n",
      "              0.1455,     -0.1321,      0.0142,     -0.1596,      0.1108,\n",
      "              0.0047,      0.1866,     -0.1278,      0.1528,      0.0268,\n",
      "              0.1498,      0.0297,      0.1508,      0.1221,      0.1419,\n",
      "             -0.0679,     -0.1309,     -0.0076,      0.0385,     -0.1213,\n",
      "             -0.1047,     -0.1712,     -0.1691,      0.1689,     -0.0561],\n",
      "        [    -0.0112,      0.0073,      0.1138,     -0.1612,     -0.1409,\n",
      "             -0.0607,     -0.1051,      0.0942,      0.1290,     -0.1771,\n",
      "             -0.1549,     -0.1778,      0.0689,      0.1469,     -0.1416,\n",
      "             -0.0845,      0.0581,     -0.1192,      0.1551,      0.0426,\n",
      "             -0.0508,      0.0119,      0.0569,     -0.0646,     -0.1415,\n",
      "              0.0012,      0.0033,      0.0037,     -0.0266,      0.1172],\n",
      "        [    -0.0509,     -0.0177,      0.0751,     -0.0994,      0.0489,\n",
      "             -0.0404,      0.1052,     -0.0950,      0.0067,      0.0179,\n",
      "             -0.1232,     -0.1328,      0.0700,     -0.0531,      0.1990,\n",
      "             -0.1804,     -0.0903,     -0.1404,      0.0729,     -0.0053,\n",
      "              0.0090,     -0.0304,     -0.1367,      0.0518,      0.0046,\n",
      "             -0.1260,      0.0687,      0.0674,     -0.1766,      0.1572],\n",
      "        [     0.1379,      0.1229,     -0.0446,      0.0073,      0.2135,\n",
      "              0.2568,     -0.1378,     -0.0825,      0.1986,      0.2015,\n",
      "              0.2008,      0.1347,      0.1714,     -0.0672,     -0.0088,\n",
      "              0.0675,      0.1287,     -0.1378,      0.1678,      0.2799,\n",
      "             -0.1261,      0.1917,     -0.0806,      0.2102,      0.1446,\n",
      "              0.1075,      0.0800,     -0.0182,      0.3273,      0.0031],\n",
      "        [    -0.0666,      0.0943,      0.0573,     -0.0635,     -0.0500,\n",
      "             -0.1615,     -0.0606,     -0.0747,     -0.0995,     -0.1649,\n",
      "              0.1214,     -0.1632,      0.1812,     -0.0958,     -0.0909,\n",
      "              0.1489,      0.0602,     -0.0613,     -0.1470,     -0.0746,\n",
      "              0.1132,     -0.0689,      0.1499,     -0.0314,     -0.0233,\n",
      "              0.0729,     -0.0268,     -0.0650,      0.1264,      0.0083],\n",
      "        [     0.0207,      0.1839,      0.2176,      0.0460,      0.0002,\n",
      "              0.1241,     -0.0160,     -0.1427,     -0.0292,      0.1639,\n",
      "              0.0702,      0.2310,     -0.0972,      0.2314,     -0.0762,\n",
      "              0.0853,     -0.0178,     -0.0881,      0.0819,      0.0213,\n",
      "             -0.1158,      0.0591,      0.1467,      0.1338,      0.1413,\n",
      "             -0.1558,      0.0996,      0.0456,      0.0404,      0.0058],\n",
      "        [     0.0230,     -0.1639,     -0.1095,      0.1489,     -0.0429,\n",
      "             -0.0367,     -0.1156,     -0.1092,      0.1438,      0.0233,\n",
      "             -0.0662,      0.0185,     -0.0913,     -0.1902,     -0.1813,\n",
      "              0.0477,     -0.1121,      0.1121,      0.1560,      0.0204,\n",
      "              0.1569,     -0.1170,      0.1485,     -0.0976,     -0.0394,\n",
      "              0.1213,     -0.0990,      0.1393,     -0.0485,     -0.0036],\n",
      "        [     0.1654,      0.1630,     -0.0603,      0.0024,      0.0365,\n",
      "              0.1663,      0.0810,      0.1420,      0.1202,     -0.1387,\n",
      "              0.1092,     -0.0104,      0.1500,      0.1315,     -0.2072,\n",
      "             -0.0089,      0.0432,      0.0419,     -0.1689,      0.0263,\n",
      "              0.0462,      0.1352,      0.0815,      0.0378,     -0.1663,\n",
      "              0.1201,     -0.1368,     -0.1842,      0.0394,     -0.0079],\n",
      "        [     0.1049,     -0.0288,     -0.0946,     -0.0201,      0.0584,\n",
      "             -0.1047,     -0.1585,      0.0882,     -0.1207,     -0.1054,\n",
      "              0.1442,     -0.1255,     -0.1480,      0.1373,     -0.1012,\n",
      "              0.0099,      0.1794,     -0.0749,      0.1701,      0.1429,\n",
      "              0.1360,      0.0502,     -0.0810,      0.1137,     -0.1267,\n",
      "             -0.1064,     -0.0891,      0.0658,      0.0209,      0.1052],\n",
      "        [     0.1200,      0.0107,      0.1064,     -0.1319,     -0.1745,\n",
      "             -0.1487,      0.1007,      0.1679,     -0.0505,      0.1010,\n",
      "             -0.1305,     -0.0034,     -0.0011,     -0.0529,     -0.0885,\n",
      "              0.0857,     -0.0159,     -0.0362,      0.1268,     -0.1386,\n",
      "              0.1192,      0.1621,     -0.1122,     -0.1730,      0.0254,\n",
      "             -0.1389,      0.0763,     -0.1456,     -0.1425,      0.0494],\n",
      "        [    -0.0468,     -0.1616,      0.0712,      0.0496,      0.0246,\n",
      "              0.1193,     -0.0831,      0.1660,     -0.1391,      0.1646,\n",
      "              0.1157,     -0.1624,     -0.0162,     -0.0982,      0.1086,\n",
      "             -0.0076,      0.1075,      0.1494,     -0.1408,      0.1110,\n",
      "             -0.1901,     -0.0493,     -0.0757,     -0.1708,     -0.1720,\n",
      "             -0.1760,     -0.1412,     -0.1607,     -0.0602,      0.0535],\n",
      "        [    -0.0293,     -0.1808,     -0.1326,      0.1153,     -0.1782,\n",
      "             -0.0898,     -0.1755,      0.0975,      0.1248,      0.0160,\n",
      "             -0.1129,     -0.0443,      0.0106,      0.0765,     -0.0718,\n",
      "             -0.0851,      0.0088,     -0.0034,      0.0017,      0.0794,\n",
      "             -0.1018,      0.0115,     -0.0014,      0.0771,     -0.1118,\n",
      "             -0.1427,     -0.1486,      0.0148,      0.1452,      0.1297],\n",
      "        [     0.0652,     -0.0372,      0.0336,     -0.1805,      0.0211,\n",
      "             -0.0603,     -0.0139,      0.1049,     -0.0628,     -0.1383,\n",
      "              0.0110,     -0.0508,     -0.0851,     -0.0558,     -0.1035,\n",
      "              0.1603,      0.0126,     -0.0969,      0.1164,      0.0282,\n",
      "              0.1048,      0.1407,      0.1804,     -0.0473,      0.0054,\n",
      "             -0.1058,      0.1666,      0.0581,     -0.0302,      0.0458],\n",
      "        [     0.1929,      0.1053,      0.1072,      0.1217,      0.0271,\n",
      "              0.0215,      0.0147,     -0.1708,      0.0749,      0.0230,\n",
      "             -0.0559,      0.0558,      0.1667,      0.0734,     -0.3883,\n",
      "              0.1811,      0.1300,      0.0655,      0.0756,      0.0400,\n",
      "             -0.1989,      0.1557,     -0.1440,     -0.0987,      0.0114,\n",
      "             -0.1417,      0.1750,     -0.0027,      0.1354,     -0.0414],\n",
      "        [    -0.1773,      0.1567,      0.0317,     -0.1178,     -0.1281,\n",
      "              0.0434,      0.1095,     -0.0485,     -0.0444,     -0.0361,\n",
      "             -0.1742,      0.1300,     -0.0158,     -0.1681,     -0.1460,\n",
      "              0.0568,     -0.1761,      0.1273,     -0.1508,     -0.0069,\n",
      "              0.0503,     -0.0872,     -0.1181,     -0.0198,      0.0324,\n",
      "             -0.0325,      0.0814,      0.0875,      0.0721,      0.1619],\n",
      "        [    -0.1116,     -0.1482,      0.0432,      0.0332,      0.1741,\n",
      "              0.1756,      0.0783,      0.1081,      0.0817,     -0.0151,\n",
      "             -0.1714,     -0.0749,     -0.1557,      0.1280,     -0.0987,\n",
      "             -0.1443,     -0.0248,      0.1242,      0.0798,      0.0654,\n",
      "             -0.0111,     -0.1694,     -0.1084,     -0.1551,     -0.0549,\n",
      "              0.1259,     -0.1216,     -0.0470,      0.1762,      0.1254],\n",
      "        [     0.0792,      0.1602,     -0.0135,     -0.0980,      0.1151,\n",
      "              0.0756,     -0.0566,      0.1409,     -0.1145,      0.0286,\n",
      "              0.1718,      0.1226,     -0.1386,      0.0547,     -0.0261,\n",
      "              0.0932,      0.1302,     -0.1292,     -0.0600,      0.1808,\n",
      "             -0.1331,      0.0114,     -0.0578,     -0.0330,     -0.0121,\n",
      "              0.1048,      0.1221,     -0.0654,      0.0844,     -0.1694],\n",
      "        [    -0.0690,     -0.0171,     -0.0066,      0.0333,      0.0100,\n",
      "             -0.0543,      0.1315,     -0.0598,      0.1458,      0.1078,\n",
      "             -0.0040,     -0.0673,      0.1124,      0.0020,     -0.0993,\n",
      "              0.0989,      0.0141,      0.1752,     -0.1796,      0.0445,\n",
      "             -0.1668,     -0.0813,     -0.0180,      0.1483,     -0.0759,\n",
      "             -0.1060,     -0.0119,     -0.0223,     -0.1447,      0.0052]])), ('layers.2.bias', tensor([ 0.1231, -0.0787, -0.1814, -0.1292,  0.1146,  0.1118,  0.1078,  0.0731,\n",
      "        -0.0480,  0.1411,  0.0516, -0.1292,  0.0109, -0.0293, -0.1201,  0.1087,\n",
      "         0.0795, -0.0781, -0.1286,  0.0773])), ('layers.4.weight', tensor([[-0.5676,  0.0463, -0.0026,  0.0374, -0.0795,  0.4237, -0.1437,  0.1482,\n",
      "         -0.0952,  0.2127, -0.0210,  0.1948,  0.1329, -0.1091, -0.0949,  0.2534,\n",
      "         -0.1640, -0.1024,  0.0329, -0.1448],\n",
      "        [ 0.5498,  0.0593, -0.2298, -0.2204,  0.2217, -0.4530, -0.1444, -0.3695,\n",
      "          0.0469, -0.1482,  0.0370, -0.2146,  0.1554, -0.1212,  0.0303, -0.2571,\n",
      "         -0.1384, -0.0065, -0.3535,  0.1861]])), ('layers.4.bias', tensor([0.0030, 0.0071]))])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"toy_model.pth\")\n",
    "\n",
    "model = NeuralNetwork(input_size=2, output_size=2)\n",
    "model.load_state_dict(torch.load(\"toy_model.pth\"))\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Mult Diff CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Generate random matrices\n",
    "# N = 50  # Change this size to test different dimensions\n",
    "# A = torch.randn(N, N)\n",
    "# B = torch.randn(N, N)\n",
    "\n",
    "# # CPU timing\n",
    "# print(\"CPU timing:\")\n",
    "# %timeit A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Generate random matrices\n",
    "# N = 50  # Change this size to test different dimensions\n",
    "# A = torch.randn(N, N).to(\"cuda\")\n",
    "# B = torch.randn(N, N).to(\"cuda\")\n",
    "\n",
    "# # CPU timing\n",
    "# print(\"GPU timing:\")\n",
    "# %timeit A @ B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
